{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcb439d1-57dc-46b3-8dcb-2470c1d2e244",
   "metadata": {},
   "source": [
    "# **Pulsar Classification Project is a classification problem.**\n",
    "# **Therefore, scaling is not considered.**\n",
    "# **However, in the improved versions of the project, scaling might be used.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d340afd-fc05-4f93-8a4a-83c5436a2b32",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d35959",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa7caa9",
   "metadata": {},
   "source": [
    "Feature Engineering can be proceduralized as follows;\n",
    "\n",
    "1. <a href='#feature_selection_section'><b>Feature Selection</b></a>\n",
    "2. <a href='#encoding_section'><b>Encoding</b></a>\n",
    "3. <a href='#feature_scaling_section'><b>Feature Scaling</b></a><br>\n",
    "    - 3.1 <a href='#predictor_scaling_section'><b>Predictor Scaling</b></a>\n",
    "    - 3.2 <a href='#target_scaling_section'><b>Target Scaling</b></a>\n",
    "4. <a href='#storing_section'><b>Storing The Data</b></a><br>\n",
    "5. <a href='#conclusion_section'><b>Conclusion</b></a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f6ff58",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947a8ab2",
   "metadata": {},
   "source": [
    "## Data Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4658af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data obtaining\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd6242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data into a data frame\n",
    "cleaned_data = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c5b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e520a30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splittng the predictors(X) and target(Y) features into two different data frames\n",
    "\n",
    "#  As an example ----> cleaned_data_X = cleaned_data[['predictor_1', 'predictor_2', 'predictor_3', ..., 'predictor_n']]\n",
    "cleaned_data_X =  ...\n",
    "\n",
    "#  As an example ----> cleaned_data_Y = cleaned_data[['target']]\n",
    "cleaned_data_Y = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79560e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dede9711",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa09fc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data_Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2b1e2b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c048fc",
   "metadata": {},
   "source": [
    "<a id='feature_selection_section'></a>\n",
    "## 1. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c826c86b",
   "metadata": {},
   "source": [
    "In this section, features, which are going to be used in the models, are selected by considering Data Exploration step results and domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c152e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is no selected feature for the PULSAR Star Classification project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3bf8e0",
   "metadata": {},
   "source": [
    "<a id='encoding_section'></a>\n",
    "## 2. Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c0db92",
   "metadata": {},
   "source": [
    "**Example Intro: There are three columns that include non-numeric categorical data in this project. Therefore, these features are converted to numeric categorical columns with nominal encoding algorithms. Since, there is no rank relationship betweeen categories of the non-numeric categorical features. Dummy Encoding is selected as the encoding algorithm.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0263130",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data_X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc30e463",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data_Y.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0988b4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding predictor variables\n",
    "#  As an example ----> encoded_data_X = pd.get_dummies(cleaned_data_X, columns = ['predictor_a', ... , 'predictor_b'], drop_first=True)\n",
    "encoded_data_X = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008309a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoded_data_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1847f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding target variable\n",
    "encoded_data_Y = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eddf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_Y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85b3120",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8e7540",
   "metadata": {},
   "source": [
    "<a id='feature_scaling_section'></a>\n",
    "## 3. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905929aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary library\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dd15f1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2e9174",
   "metadata": {},
   "source": [
    "<a id='predictor_scaling_section'></a>\n",
    "### 3.1 Predictor Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d2a872",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predictor Standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "standardizer = StandardScaler()\n",
    "standardized = encoded_data_X.copy(deep=True)\n",
    "standardized[:] = standardizer.fit_transform(standardized[:])\n",
    "standardized = pd.DataFrame(standardized)\n",
    "sns.displot(standardized, kind='kde', aspect=3, height=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae3f088",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eb4131",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predictor Normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "normalizer = MinMaxScaler()\n",
    "normalized = encoded_data_X.copy(deep=True)\n",
    "normalized[normalized.columns[:]] = normalizer.fit_transform(normalized[:])\n",
    "normalized = pd.DataFrame(normalized)\n",
    "sns.displot(normalized, kind='kde', aspect=3, height=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42e27ff",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8913561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor Maximum Absolute Scaling\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "max_abs_scaler = MaxAbsScaler()\n",
    "max_abs_scaled = encoded_data_X.copy(deep=True)\n",
    "max_abs_scaled[max_abs_scaled.columns[:]] = max_abs_scaler.fit_transform(max_abs_scaled[:])\n",
    "max_abs_scaled = pd.DataFrame(max_abs_scaled)\n",
    "sns.displot(max_abs_scaled, kind='kde', aspect=3, height=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b12db30",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d0006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor Robust Scaling\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "robust_scaler = RobustScaler()\n",
    "robust_scaled = encoded_data_X.copy(deep=True)\n",
    "robust_scaled[robust_scaled.columns[:]] = robust_scaler.fit_transform(robust_scaled[:])\n",
    "robust_scaled = pd.DataFrame(robust_scaled)\n",
    "sns.displot(robust_scaled, kind='kde', aspect=3, height=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b616c6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed73417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor Quantile Transformer Scaling\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "quantile_transformer = QuantileTransformer()\n",
    "quantile_transformed = encoded_data_X.copy(deep=True)\n",
    "quantile_transformed[quantile_transformed.columns[:]] = quantile_transformer.fit_transform(quantile_transformed[:])\n",
    "quantile_transformed = pd.DataFrame(quantile_transformed)\n",
    "sns.displot(quantile_transformed, kind='kde', aspect=3, height=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1e7c27",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0730163d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predictor Power Transformer Scaling (with Yeo-Johnson Transform)\n",
    "from sklearn.preprocessing import power_transform\n",
    "powered = encoded_data_X.copy(deep=True)\n",
    "powered[powered.columns[:]] = power_transform(powered, method='yeo-johnson')\n",
    "powered = pd.DataFrame(powered)\n",
    "sns.displot(powered, kind='kde', aspect=3, height=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf8dce2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a0ea50",
   "metadata": {},
   "source": [
    "**Example Notes: In the predictor scaling, Power transformation and Standardization results are almost the same results. Any further inference couldn't be done. However, standardization is chosen as the scaling method that is going to be used.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39563e6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9bcdce",
   "metadata": {},
   "source": [
    "<a id='target_scaling_section'></a>\n",
    "### 3.2 Target Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13f4465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f216a3f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ed2015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base-10 Logarithmic Target Scaling\n",
    "base_10_log_scaled_y = np.log10(encoded_data_Y)\n",
    "sns.displot(base_10_log_scaled_y, kind='kde', aspect=3, height=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ef8551",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb5610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base-e Logarithmic Target Scaling\n",
    "base_e_log_scaled_y = np.log(encoded_data_Y)\n",
    "sns.displot(base_e_log_scaled_y, kind='kde', aspect=3, height=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de71b281",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4741ae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "standardizer_y = StandardScaler()\n",
    "standardized_y = encoded_data_Y.copy(deep=True)\n",
    "standardized_y[:] = standardizer_y.fit_transform(standardized_y[:])\n",
    "standardized_y = pd.DataFrame(standardized_y)\n",
    "sns.displot(standardized_y, kind='kde', aspect=3, height=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c2c409",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f5f822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "normalizer_y = MinMaxScaler()\n",
    "normalized_y = encoded_data_Y.copy(deep=True)\n",
    "normalized_y[normalized_y.columns[:]] = normalizer_y.fit_transform(normalized_y[:])\n",
    "normalized_y = pd.DataFrame(normalized_y)\n",
    "sns.displot(normalized_y, kind='kde', aspect=3, height=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cedfc88",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40073119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Maximum Absolute Scaling\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "max_abs_scaler_y = MaxAbsScaler()\n",
    "max_abs_scaled_y = encoded_data_Y.copy(deep=True)\n",
    "max_abs_scaled_y[max_abs_scaled_y.columns[:]] = max_abs_scaler_y.fit_transform(max_abs_scaled_y[:])\n",
    "max_abs_scaled_y = pd.DataFrame(max_abs_scaled_y)\n",
    "sns.displot(max_abs_scaled_y, kind='kde', aspect=3, height=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3b7c14",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1384cd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Quantile Transformer Scaling\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "quantile_transformer_y = QuantileTransformer()\n",
    "quantile_transformed_y = encoded_data_Y.copy(deep=True)\n",
    "quantile_transformed_y[quantile_transformed_y.columns[:]] = quantile_transformer_y.fit_transform(quantile_transformed_y[:])\n",
    "quantile_transformed_y = pd.DataFrame(quantile_transformed_y)\n",
    "sns.displot(quantile_transformed_y, kind='kde', aspect=3, height=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df32fe87",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc19e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Power Transformer Scaling (with Yeo-Johnson Transform)\n",
    "from sklearn.preprocessing import power_transform\n",
    "powered_y = encoded_data_Y.copy(deep=True)\n",
    "powered_y[powered_y.columns[:]] = power_transform(powered_y, method='yeo-johnson')\n",
    "powered_y = pd.DataFrame(powered_y)\n",
    "sns.displot(powered_y, kind='kde', aspect=3, height=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73799632",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd5759f",
   "metadata": {},
   "source": [
    "**Example Notes: In the target scaling, Power transformation and Base-e Logarithmic transformation results are the same results. Also, Base-10 Logarithmic transformation is quite similar with Power transformation and Base-e Logarithmic transformation results. Furthermore, these results have more Gaussian shape than others. As a result Base-e Logarithmic transformation result is decided to be used because of having smaller distribution range than Base-10 Logarithmic transformation result.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3cacad",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9eaec7",
   "metadata": {},
   "source": [
    "<a id='storing_section'></a>\n",
    "## 4. Storing The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51353ae6",
   "metadata": {},
   "source": [
    "The data that is applied feature engineering method is stored in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4edb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If predictors and/or target are scaled, then choose the proper scaled algoritmh before storing the datset\n",
    "transformed_data = pd.DataFrame()\n",
    "transformed_data[standardized.columns] = standardized\n",
    "transformed_data[base_e_log_scaled_y.columns] = base_e_log_scaled_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76e55ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f644ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0683cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stroing the transformed data\n",
    "transformed_data.to_csv('Transformed_Data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22263aff",
   "metadata": {},
   "source": [
    "<a id='conclusion_section'></a>\n",
    "## 5. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2767d9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General summary of the scaled dataset is presented here. Also, necessary advices is propesed etc. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7885577",
   "metadata": {},
   "source": [
    "**Example Concsluion\\\n",
    "At first, cleaned data encoded with the dummy encoding method. Then, several scaling methods are applied on both target and predictor variables. There were no interpretable results in the predictor scaling. However, there were target scaling results that can be said as relatively near Gaussian distribution. As a conclusion, standardization is selected for the predictor scaling method and Base-e Logarithmic is selected for the target scaling method.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fac197",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
